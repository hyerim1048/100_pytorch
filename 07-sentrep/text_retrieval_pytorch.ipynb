{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-retrieval-pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hyerim1048/100_pytorch/blob/master/07-sentrep/text_retrieval_pytorch.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "bRMwb2bicQuW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "f743ed07-caf1-4a53-eea6-04685c68bbd8"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.2.3)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.23.4)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.4.16)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c_IcS8bjcUro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UH8U_w6NcZ0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# format of files: each line is \"word1 word2 ...\" aligned line-by-line\n",
        "train_src_file = \"nn4nlp-code/data/parallel/train.ja\"\n",
        "train_trg_file = \"nn4nlp-code/data/parallel/train.en\"\n",
        "dev_src_file = \"nn4nlp-code/data/parallel/dev.ja\"\n",
        "dev_trg_file = \"nn4nlp-code/data/parallel/dev.en\"\n",
        "\n",
        "w2i_src = defaultdict(lambda: len(w2i_src))\n",
        "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
        "\n",
        "def read(fname_src, fname_trg):\n",
        "    \"\"\"\n",
        "    Read parallel files where each line lines up\n",
        "    \"\"\"\n",
        "    with open(fname_src, \"r\") as f_src, open(fname_trg, \"r\") as f_trg:\n",
        "        for line_src, line_trg in zip(f_src, f_trg):\n",
        "            sent_src = [w2i_src[x] for x in line_src.strip().split()]\n",
        "            sent_trg = [w2i_trg[x] for x in line_trg.strip().split()]\n",
        "            yield (sent_src, sent_trg)\n",
        "\n",
        "# Read the data\n",
        "train = list(read(train_src_file, train_trg_file))\n",
        "unk_src = w2i_src[\"<unk>\"]\n",
        "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
        "unk_trg = w2i_trg[\"<unk>\"]\n",
        "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
        "nwords_src = len(w2i_src)\n",
        "nwords_trg = len(w2i_trg)\n",
        "dev = list(read(dev_src_file, dev_trg_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDB0DE1wpkaN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_src, train_trg = zip(*train)\n",
        "dev_src, dev_trg = zip(*dev)\n",
        "train_src_len, train_trg_len = map(len, train_src), map(len, train_trg)\n",
        "dev_src_len, dev_trg_len = map(len, dev_src), map(len, dev_trg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MRvDUQnMrxAV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_src_len, train_trg_len = map(len, train_src), map(len, train_trg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THGxy6ICsCBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zQVp5iDarj-t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ParallelCorpus(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "    \n",
        "  def __getitem__(self, ix):\n",
        "    return torch.LongTensor(self.data[ix][0]), torch.LongTensor(self.data[ix][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qM-sV2qgvWQ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def my_collate_fn(batch):\n",
        "  src, trg = zip(*batch)\n",
        "  src_len, trg_len = list(map(len, src)), list(map(len, trg))\n",
        "  src_maxlen, trg_maxlen = max(src_len), max(trg_len)\n",
        "  \n",
        "  src = torch.stack([F.pad(e, (0, src_maxlen-len(e))) for e in src])\n",
        "  trg = torch.stack([F.pad(e, (0, trg_maxlen-len(e))) for e in trg])\n",
        "  \n",
        "  return src, trg, torch.LongTensor(src_len), torch.LongTensor(trg_len)\n",
        "\n",
        "# my_collate_fn([train_corpus[i] for i in range(4)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1mqiM59tN-5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bWPl_CVJccIi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "EMBED_SIZE = 64\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yr5TpPhOMSSB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_corpus = ParallelCorpus(train)\n",
        "train_loader = DataLoader(train_corpus, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, collate_fn=my_collate_fn)\n",
        "\n",
        "dev_corpus = ParallelCorpus(dev)\n",
        "dev_loader = DataLoader(dev_corpus, batch_size=BATCH_SIZE, shuffle=False, num_workers=1, collate_fn=my_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z2zhP_PkMpoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4603e480-6947-4e95-ff3d-e64cacad5032"
      },
      "cell_type": "code",
      "source": [
        "for e in dev_loader:\n",
        "  print(e[1].shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 22])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X4MwALjCceWs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SentRep(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, batch_size):\n",
        "    super(SentRep, self).__init__()\n",
        "    \n",
        "    self.hid_dim = hid_dim\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    self.embeddings = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "    self.lstm = nn.LSTM(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "  def init_hidden(self, bs):\n",
        "    # Before we've done anything, we dont have any hidden state.\n",
        "    # Refer to the Pytorch documentation to see exactly\n",
        "    # why they have this dimensionality.\n",
        "    # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "    return (torch.zeros(2, bs, self.hid_dim).cuda(),\n",
        "            torch.zeros(2, bs, self.hid_dim).cuda())\n",
        "  def forward(self, x, x_len):\n",
        "    h0 = self.init_hidden(x_len.shape[0])\n",
        "    encoded = self.embeddings(x)\n",
        "    output, _ = self.lstm(encoded, h0)\n",
        "    return torch.stack([output[i, x_len[i]-1] for i in range(len(x_len))])\n",
        "#     return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "krjnrNNb075C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_score(x, y):\n",
        "  m = x @ y.transpose(0, 1)\n",
        "  score = torch.max(1 + m - m.diag(), torch.cuda.FloatTensor([0]))\n",
        "  for i in range(m.shape[0]):\n",
        "    score[i,i] = 0.\n",
        "  \n",
        "  return score.sum() / m.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wYGOujGjILH3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def index_corpus(loader):\n",
        "  souts, touts = [], []\n",
        "  for batch_i, (s, t, sl, tl) in enumerate(loader):\n",
        "    sout = src_reps(s.cuda(), sl.cuda())\n",
        "    tout = trg_reps(t.cuda(), tl.cuda())\n",
        "    souts.append(sout)\n",
        "    touts.append(tout)\n",
        "  \n",
        "  return torch.cat(souts, 0).cpu().detach().numpy(), torch.cat(touts, 0).cpu().detach().numpy()\n",
        "\n",
        "def retrieve(src, db_mtx):\n",
        "    scores = np.dot(db_mtx, src)\n",
        "    ranks = np.argsort(-scores)\n",
        "    return ranks, scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwx3_8Foexpp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "src_reps = SentRep(nwords_src, EMBED_SIZE, HIDDEN_SIZE, BATCH_SIZE).cuda()\n",
        "trg_reps = SentRep(nwords_trg, EMBED_SIZE, HIDDEN_SIZE, BATCH_SIZE).cuda()\n",
        "\n",
        "trainer = torch.optim.Adam(list(src_reps.parameters()) + list(trg_reps.parameters()), lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WayQ9h5fPTP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch_i in range(100):\n",
        "  src_reps.train()\n",
        "  trg_reps.train()\n",
        "  total_loss = 0.\n",
        "  for batch_i, (s, t, sl, tl) in enumerate(train_loader):\n",
        "    sout = src_reps(s.cuda(), sl.cuda())\n",
        "    tout = trg_reps(t.cuda(), tl.cuda())\n",
        "\n",
        "    score = calc_score(sout, tout)\n",
        "    total_loss += score.item()\n",
        "\n",
        "    trainer.zero_grad()\n",
        "    score.backward()\n",
        "    trainer.step()\n",
        "  \n",
        "  print(\"epoch %r | loss: %f\" % (epoch_i, total_loss / batch_i))\n",
        "  \n",
        "  # Perform evaluation \n",
        "  src_reps.eval()\n",
        "  trg_reps.eval()\n",
        "  dev_start = time.time()\n",
        "  rec_at_1, rec_at_5, rec_at_10 = 0, 0, 0\n",
        "  src_mtx, trg_mtx = index_corpus(dev_loader)\n",
        "  for i in range(trg_mtx.shape[0]):\n",
        "    ranks, scores = retrieve(src_mtx[i], trg_mtx)\n",
        "    if ranks[0] == i: rec_at_1 += 1\n",
        "    if i in ranks[:5]: rec_at_5 += 1\n",
        "    if i in ranks[:10]: rec_at_10 += 1\n",
        "  print(\"epoch %r: dev recall@1=%.2f%% recall@5=%.2f%% recall@10=%.2f%%\" % (epoch_i, rec_at_1/len(dev)*100, rec_at_5/len(dev)*100, rec_at_10/len(dev)*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vcdDqZwrn-6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35bb1835-d8bc-4502-9128-2d641524fb32"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 10,  17,  22,  ...,  12,  13,  11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "slUo8E-opVAx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}